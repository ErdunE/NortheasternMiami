CS 6140 Machine Learning
Assignment 1 â€“ Machine Learning in Policing
Name: Erdun E  
Professor: Dr. Tala Talaei Khoei
Date: Sep 09, 2025

Q1: Define ethical in ML briefly. And is it ethical to deploy the Crime Predict framework when its data is demonstrably skewed?
A1: First of all, the ethical in machine learning, means develop and use models in ways that must respect fairness, human rights, and responsibility. An ethical system which built by ML, should not hurt people or any group unfairly due to biased data or design. So, I do not think it is ethical to use CrimePredict if the training data is biased, becuase this can cause to unfair targeting for some neighborhoods or people or groups. Even if the system helps reduce time and crime, but it still create discrimination and fear. A great ML system should be tested for fairness before use!
    

Q2: Discuss whether predictive police are fundamentally flawed if crime data is inherently biased and propose at least two concrete ethical guidelines for ML-based police systems that impact public safety.
A2: If tge crime data is already biased, then I believe predictive policing is fundamentally flawed. Biased data leads to biased predictions, which can unfairly target certain communities and increase mistrust with the police. To improve the ML-based police systems, my suggestion of the concrete ethical guidelines are: 
    1. always human review and make decision before taking any real-world actions.
    2. Audit and retrain the models periodly and using balanced and updated datas.
    In this case, I think these steps could help reduce hurt and improve the fairness in this system.

Q3: Should third-party contractors be legally and morally responsible for discriminatory results?
A3: This is complex question, in my view, third-party contractors should at least be morally responsible for any discriminatory results, because they create the tools and must ensure the tools are safe and fair. But for legal responsibilities may have to depend on the contract or law that if police use the tools in wrong way, it may not be the third-party contractor's fault.But if the third-party contractors sold a biased model and knew it could hurt people, then take any legal action may be faired. So in both cases and in my opinion, tbe transparency and testing are important,third-party contractors should not only sell any tools, but also take responsibility for its impact.

Q4: Define briefly fairness in ML. And, how can we make sure a ML-based Policing frame-work treats all groups of people fairly?
A4: The fairness in ML means treat all people or group equally, without bias or discrimination. A fair ML system should gives similar results to people in different races, genders, or communities when they are in simi;ar situations. To ensure the fairness, we can use some techniques like demographic parity or equal opportunity testing, these techs check if the model treats different gourps equally. Also we should audit the training data and remove unfair patterns. Last, may be include some community representatives is a good way in system evaluation.

Q5: Define briefly transparency in ML. Why should the public know how ML-based policing tools used make their predictions?
A5: Transparency in ML means the system's process and decision process can be explained and understood. People has the right to know how the model works and why the certain results be gave. If people understands how predictions are made, they can trust the system more. And it also helps find problems, like bias or unfair rules. If without transparency, the system may be seems like as a black box that no one can question it.
